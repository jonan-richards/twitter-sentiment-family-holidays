{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from calendar import monthrange\n",
    "from os.path import exists\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import logging\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_baseline_dates(year, seed):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    return [\n",
    "        '{year}-{month:02}-{day:02}'.format(\n",
    "            year=year,\n",
    "            month=month,\n",
    "            day=rng.integers(1, monthrange(year, month)[1], size=1, endpoint=True)[0]\n",
    "        )\n",
    "        for month in range(1, 12 + 1)\n",
    "    ]\n",
    "\n",
    "# Generated with generate_baseline_dates(2018, 12345)\n",
    "baseline_dates = [\n",
    "    '2018-01-22',\n",
    "    '2018-02-07',\n",
    "    '2018-03-25',\n",
    "    '2018-04-10',\n",
    "    '2018-05-07',\n",
    "    '2018-06-24',\n",
    "    '2018-07-20',\n",
    "    '2018-08-21',\n",
    "    '2018-09-30',\n",
    "    '2018-10-13',\n",
    "    '2018-11-26',\n",
    "    '2018-12-11'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['family']\n",
    "keywords = {keyword: f'my {keyword}' for keyword in keywords}\n",
    "\n",
    "# Dates per holiday\n",
    "holidays = {\n",
    "    'Thanksgiving': ['2018-11-22', '2018-11-23'],\n",
    "    'Christmas': ['2018-12-24', '2018-12-25', '2018-12-26'],\n",
    "    'New Year\\'s': ['2018-12-31', '2019-01-01'],\n",
    "    'Baseline': baseline_dates\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snscrape.modules.twitter import TwitterSearchScraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format(tweet):\n",
    "    return [tweet.id, tweet.date.strftime('%Y-%m-%d %H:%M'), tweet.user.id, tweet.content.replace('\\n', '\\\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(path, keywords, date_start, date_end, limit = -1):\n",
    "    with open(path, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file, delimiter='\\t', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for i, tweet in enumerate(TwitterSearchScraper(\n",
    "            '(' + ' OR '.join(keywords) + ') lang:en until:' + date_end.strftime('%Y-%m-%d') + ' since:' + date_start.strftime('%Y-%m-%d') + ''\n",
    "        ).get_items()):\n",
    "            if limit != -1 and i >= limit:\n",
    "                break\n",
    "\n",
    "            writer.writerow(format(tweet))\n",
    "\n",
    "            count = i + 1\n",
    "            if count % 100 == 0:\n",
    "                print('\\r', '{0} tweets scraped'.format(count), end='')\n",
    "\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, keyword in keywords.items():\n",
    "    for _, dates in holidays.items():\n",
    "        for date in dates:\n",
    "            datetime = datetime.strptime(date, '%Y-%m-%d')\n",
    "\n",
    "            file = f'data/tweets/{label}.tweets.{date}.tsv'\n",
    "            if exists(file):\n",
    "                continue\n",
    "\n",
    "            print('Starting: {0}'.format(date))\n",
    "\n",
    "            scrape(\n",
    "                file,\n",
    "                [keyword],\n",
    "                datetime,\n",
    "                datetime + timedelta(1)\n",
    "            )\n",
    "\n",
    "            print('Done')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering and inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(include_labels=False):\n",
    "    data = []\n",
    "\n",
    "    for key in keywords.keys():\n",
    "        for holiday, dates in holidays.items():\n",
    "            for date in dates:\n",
    "                tweets_path = f'data/tweets/{key}.tweets.{date}.tsv'\n",
    "                tweets = pd.read_csv(tweets_path, names=['id', 'date_time', 'user_id', 'text'], sep='\\t').set_index('id')\n",
    "                tweets['text'] = tweets['text'].apply(lambda text: text.replace('\\\\n', '\\n'))\n",
    "                tweets['date'] = tweets['date_time'].apply(lambda x: x.split(' ')[0])\n",
    "                tweets['holiday'] = holiday\n",
    "\n",
    "                if include_labels:\n",
    "                    labels_path = f'data/tweets/{key}.labels.{date}.tsv'\n",
    "                    labels = pd.read_csv(labels_path, names=['id', 'label'], sep='\\t').set_index('id')\n",
    "                    tweets = tweets.join(labels)\n",
    "        \n",
    "                data.append(tweets)\n",
    "\n",
    "    return pd.concat(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(data, day_threshold=2, duplicate_n_day_threshold=2):\n",
    "    keyword_pattern = '(?:' + '|'.join(['[^a-z]*'.join([f'#?{keyword}' for keyword in phrase.split(' ')]) for phrase in keywords.values()]) + ')'\n",
    "    quotes = '\"“”'\n",
    "\n",
    "    # Filter out tweets not containing any keywords\n",
    "    remove = ~data['text'].str.contains(keyword_pattern, case=False)\n",
    "    data = data[~remove]\n",
    "    logging.info(f'Filtering {sum(remove)} tweets not containing keywords')\n",
    "    \n",
    "    # Filter out tweets where the keywords only occurs within quotes,\n",
    "    # e.g. if the keywords are not preceded by an even (or zero) number of quotes\n",
    "    remove = ~data['text'].str.contains(f'^(?:(?:[^{quotes}]*[{quotes}]){{2}})*[^{quotes}]*{keyword_pattern}', case=False)\n",
    "    data = data[~remove]\n",
    "    logging.info(f'Filtering {sum(remove)} tweets with keywords within quotes')\n",
    "\n",
    "    # Filter out tweets that are duplicate on a day for a user\n",
    "    before = data.shape[0]\n",
    "    data = data.sort_values('date_time').groupby(['date', 'user_id', 'text']).head(1)\n",
    "    after = data.shape[0]\n",
    "    logging.info(f'Filtering {before - after} tweets duplicate for a day')\n",
    "\n",
    "    # Filter out tweets that are posted by a user on more than duplicate_n_day_threshold dates\n",
    "    remove = data['user_id'].isin(set(\n",
    "        data\n",
    "            .groupby(['user_id', 'text'])\n",
    "            .filter(lambda x: x['date'].count() > duplicate_n_day_threshold)['user_id']\n",
    "    ))\n",
    "    data = data[~remove]\n",
    "    logging.info(f'Filtering {sum(remove)} tweets from a user posting duplicated tweets over the duplicate_n_day_threshold')\n",
    "\n",
    "    # Filter out tweets that are not a user's first per_day_threshold tweets of a day\n",
    "    before = data.shape[0]\n",
    "    data = data.sort_values('date_time').groupby(['date', 'user_id']).head(day_threshold)\n",
    "    after = data.shape[0]\n",
    "    logging.info(f'Filtering {before - after} tweets over the per_day_threshold')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load()\n",
    "data.sample(1000, random_state=12345).to_csv('data/sample.inspection.100.tsv', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport classify_sentiment\n",
    "from classify_sentiment import SentimentClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = SentimentClassification(\n",
    "    temp_dir='data/temp',\n",
    ")\n",
    "classification.load_pipeline('models/classifier.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = filter(load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, keyword in keywords.items():\n",
    "    for _, dates in holidays.items():\n",
    "        for date in dates:\n",
    "            labels_path = f'data/tweets/{key}.labels.{date}.tsv'\n",
    "            if exists(labels_path):\n",
    "                continue\n",
    "            \n",
    "            chunk = data[data['date'] == date].reset_index()[['id', 'text']].copy()\n",
    "\n",
    "            logging.info(f'Classifying: {key}/{date} ({chunk.shape[0]} tweets)')\n",
    "\n",
    "            chunk['label'] = classification.predict(chunk['text'])\n",
    "            chunk[['id', 'label']].to_csv(labels_path, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install tikzplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = filter(load(include_labels=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "\n",
    "label_map = {\n",
    "    0: 'negative',\n",
    "    1: 'neutral',\n",
    "    2: 'positive'\n",
    "}\n",
    "\n",
    "for key, keyword in keywords.items():\n",
    "    for holiday, dates in holidays.items():\n",
    "        for date in dates:\n",
    "            labels_path = f'data/tweets/{key}.labels.{date}.tsv'\n",
    "            labels = pd.read_csv(labels_path, names=['id', 'label'], sep='\\t')[['label']]\n",
    "    \n",
    "            counts = labels.value_counts()\n",
    "\n",
    "            results = pd.concat([\n",
    "                results, \n",
    "                pd.DataFrame([{\n",
    "                    'date': date,\n",
    "                    'holiday': holiday,\n",
    "                    **{f'{label}_count': counts.loc[(k,)] for k, label in label_map.items()}\n",
    "                }]).set_index('date')\n",
    "            ])\n",
    "\n",
    "results_dates = results\n",
    "results_dates['n_tweets'] = sum([results_dates[f'{label}_count'] for label in label_map.values()])\n",
    "for label in label_map.values():\n",
    "    results_dates[label] = round(results_dates[f'{label}_count'] / results_dates['n_tweets'] * 100, 2)\n",
    "\n",
    "results_holidays = results_dates\n",
    "results_holidays_groups = results_holidays.reset_index().groupby(['holiday'], sort=False)\n",
    "results_holidays = results_holidays_groups.agg({\n",
    "    **{f'{label}_count': 'sum' for label in label_map.values()},\n",
    "    **{f'{label}': ['min', 'max'] for label in label_map.values()},\n",
    "    'n_tweets': ['sum', 'min', 'max', 'mean']\n",
    "})\n",
    "results_holidays.columns = results_holidays.columns.get_level_values(0) + '_' + results_holidays.columns.get_level_values(1)\n",
    "results_holidays = results_holidays.rename(columns={\n",
    "    **{f'{label}_count_sum': f'{label}_count' for label in label_map.values()},\n",
    "    'n_tweets_sum': 'n_tweets',\n",
    "    'n_tweets_mean': 'n_tweets_avg',\n",
    "})\n",
    "\n",
    "for label in label_map.values():\n",
    "    results_holidays[label] = round(results_holidays[f'{label}_count'] / results_holidays['n_tweets'] * 100, 2)\n",
    "\n",
    "results_holidays['n_dates'] = results_holidays_groups.size()\n",
    "\n",
    "results_holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "mpl.use('pgf')\n",
    "import matplotlib.pyplot as plt\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "fig, ax = plt.subplots(ncols=2, sharey=True)\n",
    "\n",
    "x = np.arange(results_holidays.shape[0])\n",
    "width = 0.75/len(label_map)\n",
    "\n",
    "# Sentiment distribution\n",
    "for i, (key, label) in enumerate(reversed(label_map.items())):\n",
    "    ax[0].barh(\n",
    "        x + (i - (len(label_map) -1) / 2) * width,\n",
    "        results_holidays[label],\n",
    "        height=width,\n",
    "        label=label,\n",
    "        xerr=np.stack((\n",
    "            results_holidays[label] - results_holidays[f'{label}_min'], \n",
    "            results_holidays[f'{label}_max'] - results_holidays[label]\n",
    "        )),\n",
    "        capsize=2\n",
    "    )\n",
    "\n",
    "ax[0].set_title('Sentiment distribution')\n",
    "ax[0].set_xlabel('\\\\% of tweets')\n",
    "ax[0].set_ylabel('')\n",
    "ax[0].yaxis.set_ticks(x, holidays.keys())\n",
    "ax[0].tick_params(axis='both', which='both', length=0)\n",
    "ax[0].legend()\n",
    "ax[0].xaxis.get_major_ticks()[0].set_visible(False)\n",
    "ax[0].set_xlim(100, 0)\n",
    "ax[0].invert_yaxis()\n",
    "\n",
    "# Total number\n",
    "ax[1].barh(\n",
    "    x,\n",
    "    results_holidays['n_tweets_avg'],\n",
    "    height=.4, color='r',\n",
    "    xerr=np.stack((\n",
    "        results_holidays['n_tweets_avg'] - results_holidays[f'n_tweets_min'], \n",
    "        results_holidays[f'n_tweets_max'] - results_holidays['n_tweets_avg']\n",
    "    )),\n",
    "    capsize=2\n",
    ")\n",
    "\n",
    "ax[1].set_title('Tweets per day')\n",
    "ax[1].set_xlabel('\\\\# of tweets')\n",
    "ax[1].set_ylabel('')\n",
    "ax[1].tick_params(axis='both', which='both', length=0)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.subplots_adjust(wspace=0)\n",
    "plt.savefig('data/results.pgf', format='pgf')\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results aggregated per holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "lines.append('\\\\begin{tabular}{r|ccc|ccc|ccc|cccc}')\n",
    "lines.append('\\\\toprule')\n",
    "\n",
    "lines.append(' & '.join([\n",
    "    '',\n",
    "    *[\n",
    "        f'\\\\multicolumn{{3}}{{|c}}{{\\\\% {label}}}'\n",
    "        for label in label_map.values()\n",
    "    ],\n",
    "    '\\\\multicolumn{4}{|c}{\\\\# tweets}',\n",
    "]) + '\\\\\\\\')\n",
    "lines.append(' & '.join([\n",
    "    'Holiday',\n",
    "    *[\n",
    "        'of total',\n",
    "        'min.',\n",
    "        'max.'\n",
    "    ] * 3,\n",
    "    'total',\n",
    "    'avg.',\n",
    "    'min.',\n",
    "    'max.',\n",
    "]) + '\\\\\\\\')\n",
    "\n",
    "lines.append('\\\\midrule')\n",
    "\n",
    "for holiday in holidays.keys():\n",
    "    lines.append('\\t' + ' & '.join([\n",
    "        holiday,\n",
    "        *[\n",
    "            '{0:.2f}'.format(results_holidays.loc[holiday][f'{label}{suffix}'])\n",
    "            for label in label_map.values()\n",
    "            for suffix in ['', '_min', '_max']\n",
    "        ],\n",
    "        *[\n",
    "            '{0:.0f}'.format(results_holidays.loc[holiday][f'n_tweets{suffix}'])\n",
    "            for suffix in ['', '_avg', '_min', '_max']\n",
    "        ],\n",
    "    ]) + '\\\\\\\\')\n",
    "\n",
    "lines.append('\\\\bottomrule')\n",
    "lines.append('\\\\end{tabular}')\n",
    "\n",
    "table = '\\n'.join(lines)\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results aggregated per holiday (vertical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "lines.append(f'\\\\begin{{tabular}}{{r|{\"c\" * len(holidays)}}}')\n",
    "lines.append('\\\\toprule')\n",
    "\n",
    "lines.append(' & '.join([\n",
    "    '',\n",
    "    *[\n",
    "        f'\\\\makebox[1em][l]{{\\\\rotatebox{{45}}{{{holiday}}}}}'\n",
    "        for holiday in holidays.keys()\n",
    "    ],\n",
    "]) + '\\\\\\\\')\n",
    "\n",
    "lines.append('\\\\midrule')\n",
    "\n",
    "grouped_rows = {\n",
    "    **{\n",
    "        label.capitalize(): {\n",
    "            '\\\\% of total': (label, lambda x: '{0:.2f}'.format(x)),\n",
    "            'Min. \\\\%': (f'{label}_min', lambda x: '{0:.2f}'.format(x)),\n",
    "            'Max. \\\\%': (f'{label}_max', lambda x: '{0:.2f}'.format(x)),\n",
    "        }\n",
    "        for label in ['positive', 'neutral', 'negative']\n",
    "    },\n",
    "    '\\\\# tweets': {\n",
    "        'Total': ('n_tweets', lambda x: '{0:,.0f}'.format(x)),\n",
    "        'Avg.': ('n_tweets_avg', lambda x: '{0:,.0f}'.format(x)),\n",
    "        'Min.': ('n_tweets_min', lambda x: '{0:,.0f}'.format(x)),\n",
    "        'Max.': ('n_tweets_max', lambda x: '{0:,.0f}'.format(x)),\n",
    "    }\n",
    "}\n",
    "\n",
    "for i, (header, rows) in enumerate(grouped_rows.items()):\n",
    "    if i != 0:\n",
    "        lines.append('\\\\midrule')\n",
    "\n",
    "    lines.append(f'\\\\multicolumn{{{len(holidays) + 1}}}{{c}}{{{header}}}\\\\\\\\')\n",
    "    lines.append('\\\\midrule')\n",
    "\n",
    "    for label, (key, format) in rows.items():\n",
    "        lines.append('\\t' + ' & '.join([\n",
    "            label,\n",
    "            *[\n",
    "                format(results_holidays.loc[holiday][key])\n",
    "                for holiday in holidays.keys()\n",
    "            ]\n",
    "        ]) + '\\\\\\\\')\n",
    "\n",
    "lines.append('\\\\bottomrule')\n",
    "lines.append('\\\\end{tabular}')\n",
    "\n",
    "table = '\\n'.join(lines)\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results expanded per date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "lines.append('\\\\begin{tabular}{r|cccc}')\n",
    "lines.append('\\\\toprule')\n",
    "\n",
    "lines.append(' & '.join([\n",
    "    'Date',\n",
    "    '\\\\% neg.',\n",
    "    '\\\\% neu.',\n",
    "    '\\\\% pos.',\n",
    "    '\\\\# tweets',\n",
    "]) + '\\\\\\\\')\n",
    "\n",
    "lines.append('\\\\midrule')\n",
    "\n",
    "for i, (holiday, dates) in enumerate(holidays.items()):\n",
    "    if i != 0:\n",
    "        lines.append('\\\\midrule')\n",
    "\n",
    "    lines.append(f'\\\\multicolumn{{5}}{{c}}{{{holiday}}}\\\\\\\\')\n",
    "    lines.append('\\\\midrule')\n",
    "\n",
    "    for date in dates:\n",
    "        lines.append('\\t' + ' & '.join([\n",
    "            date,\n",
    "            '{0:.2f}'.format(results_dates.loc[date]['negative']),\n",
    "            '{0:.2f}'.format(results_dates.loc[date]['neutral']),\n",
    "            '{0:.2f}'.format(results_dates.loc[date]['positive']),\n",
    "            '{}'.format(results_dates.loc[date]['n_tweets']),\n",
    "        ]) + '\\\\\\\\')\n",
    "\n",
    "lines.append('\\\\bottomrule')\n",
    "lines.append('\\\\end{tabular}')\n",
    "\n",
    "table = '\\n'.join(lines)\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classify_sentiment import evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample validation tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = filter(load(include_labels=True))\n",
    "data['manual_label'] = '###'\n",
    "data.sample(100, random_state=123456)[['label', 'manual_label', 'text']].to_csv('data/sample.validation.100.tsv', sep='\\t', header=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and evaluate manually labeled validation tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('data/sample.validation.labeled.100.tsv', names=['id', 'predicted', 'manual', 'text'], dtype={'predicted': np.int32, 'manual': np.int32}, sep='\\t')[['predicted', 'manual']]\n",
    "print(evaluate(list(results['manual']), list(results['predicted'])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "lines.append('\\\\begin{tabular}{l|ccc}')\n",
    "lines.append('\\\\toprule')\n",
    "\n",
    "lines.append(' & '.join([\n",
    "    '',\n",
    "    '\\\\multicolumn{3}{c}{Manual}',\n",
    "]) + '\\\\\\\\')\n",
    "lines.append(' & '.join([\n",
    "    'Classifier',\n",
    "    *[label_manual.capitalize() for label_manual in reversed(label_map.values())],\n",
    "]) + '\\\\\\\\')\n",
    "\n",
    "lines.append('\\\\midrule')\n",
    "\n",
    "for key_classifier, label_classifier in reversed(label_map.items()):\n",
    "    lines.append('\\t' + ' & '.join([\n",
    "        label_classifier.capitalize(),\n",
    "        *[\n",
    "            str(sum((results['predicted'] == key_classifier) & (results['manual'] == key_manual)))\n",
    "            for key_manual in reversed(label_map.keys())\n",
    "        ],\n",
    "    ]) + '\\\\\\\\')\n",
    "\n",
    "lines.append('\\\\bottomrule')\n",
    "lines.append('\\\\end{tabular}')\n",
    "\n",
    "table = '\\n'.join(lines)\n",
    "\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "txmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3c288e7661af7b23b134bc6bbf12205a40e7750452aaaf289c8024ad83b85b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
